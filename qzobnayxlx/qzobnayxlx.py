# Import necessary modules from the langchain package
from langchain.docstore.document import Document
from langchain.document_loaders.pdf import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.vectorstores.docarray import DocArrayInMemorySearch
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain

from ga_model import watsonx_llm

# Load the content of PDF documents
loader = PyPDFLoader("test.pdf")
documents = loader.load()

# Load the embedding model
embeddings_gen = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")


# Class to create hashable instances of Langchain's Document with an associated score
class HashableDocument:
    def __init__(self, document, score):
        self.document = document
        self.score = score

    def __hash__(self):
        # Hash based on the document's content and score
        return hash((self.document.page_content, self.score))

    def __eq__(self, other):
        # Check for equality based on the document's content
        return (
            isinstance(other, HashableDocument)
            and self.document.page_content == other.document.page_content
        )


# Split the documents into smaller chunks to facilitate processing and indexing
def split_documents(documents, chunk_size=300, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    docs = splitter.split_documents(documents)
    return docs


# Create a vector store for indexing - In-memory DocArray storage for exact search
def get_index_store(docs):
    index_store = DocArrayInMemorySearch.from_documents(docs, embeddings_gen)
    return index_store


# Get similar chunks of text for a single query using the provided index store
def get_similiar_docs(index_store, query, k=5):
    similar_docs = index_store.similarity_search_with_score(query, k=k)
    return similar_docs


# Retrieve similar documents for all queries generated by the language model
def get_all_similar_docs(index_store, queries):
    all_similar_docs = {}
    for query in queries:
        search_results = get_similiar_docs(index_store, query)
        all_similar_docs[query] = search_results
    return all_similar_docs


# Perform reciprocal rank fusion on the search results from different queries.
def reciprocal_rank_fusion(search_results_dict, k=60, top_n_nodes=5):
    fused_scores = {}
    print("Initial individual search result ranks:")

    for query_description, doc_scores_list in search_results_dict.items():
        print(f"For query '{query_description}': {doc_scores_list}")
    print()

    for query_description, doc_scores_list in search_results_dict.items():
        for rank, (doc, score) in enumerate(
            sorted(doc_scores_list, key=lambda x: x[1], reverse=True)
        ):
            hashable_doc = HashableDocument(doc, score)
            if hashable_doc.document.page_content not in fused_scores:
                fused_scores[hashable_doc.document.page_content] = 0
            previous_score = fused_scores[hashable_doc.document.page_content]
            fused_scores[hashable_doc.document.page_content] += 1 / (rank + k)
            print(
                f"Updating score for {doc} from {previous_score} to {fused_scores[hashable_doc.document.page_content]} based on rank {rank} in query '{query_description}'"
            )

    reranked_results = [
        Document(
            page_content=page_content,
            metadata={"source": hashable_doc.document.metadata["source"]},
        )
        for page_content, score in sorted(
            fused_scores.items(), key=lambda x: x[1], reverse=True
        )
    ]
    print("\nFinal reranked results:", reranked_results)
    return reranked_results[:top_n_nodes]


# Define the original user query and the number of queries to generate
original_input_user_query = "Dam projects and environmental implications?"
num_queries = 4


# Define the prompt template for generating search queries based on the original input query
query_gen_prompt_str = """[INST] <<SYS>>
You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the given input query
Only provide the output containing the new similar search queries. Exclude any prefix or suffix statements.
<</SYS>>
Input Query:
{query}
[/INST] 
"""

# Instantiate a PromptTemplate object using the prompt template string
query_gen_prompt = PromptTemplate.from_template(query_gen_prompt_str)

# Generate the input prompt for the LLM based on the original query and the number of queries to generate
input_prompt = query_gen_prompt.format(
    num_queries=num_queries, query=original_input_user_query
)


# Generate search queries using watsonx.ai LLM for the given input prompt
results = watsonx_llm.generate([input_prompt])
queries = results.generations[0][0].text.split("\n")

docs = split_documents(documents)
index_store = get_index_store(docs)
all_similar_docs = get_all_similar_docs(index_store, queries)
final_docs = reciprocal_rank_fusion(all_similar_docs)

# Concatenate the content of each retrieved document
output_context = ""
for doc in final_docs:
    output_context += doc.page_content

# Load a question-answering chain, run it with retrived chunks of data and user query, and return the relevant response from the LLM
chain = load_qa_chain(llm=watsonx_llm, verbose=True)
answer = chain.run(input_documents=final_docs, question=original_input_user_query)
print(answer.strip())