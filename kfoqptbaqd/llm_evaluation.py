import os
# from dotenv import load_dotenv
from ibm_watson_machine_learning.foundation_models import Model
from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams
#from langchain_community.llms import WatsonxLLM
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import asyncio

# Initialize LLM

async def llm_evaluation(watsonai_apikey, ibm_cloud_url, project_id, questions, expected_answers, llm_answers):
    try:
        parameters = {
        GenParams.DECODING_METHOD: "sample",
        GenParams.MAX_NEW_TOKENS: 500,
        GenParams.MIN_NEW_TOKENS: 1,
        GenParams.TEMPERATURE: 0.2,
        GenParams.RANDOM_SEED: 42,
        GenParams.REPETITION_PENALTY: 1.1,
        GenParams.TOP_K: 50,
        GenParams.TOP_P: 1,
        }
        
        # watsonx_llm = WatsonxLLM(
        #     # model_id="meta-llama/llama-2-70b-chat",
        #     model_id="ibm/granite-13b-instruct-v2",
        #     apikey=watsonai_apikey,
        #     url=ibm_cloud_url,
        #     project_id=project_id,
        #     params=parameters,
        # )
        
        
        model = Model(
        model_id="ibm/granite-13b-instruct-v2",
        credentials={
        "apikey": watsonai_apikey,
        "url": ibm_cloud_url
        },
        params=parameters,
        project_id=project_id
        )

        watsonx_llm = WatsonxLLM(model=model)


        llm_evals = []
        for question, expected_answer, llm_answer in zip(questions, expected_answers, llm_answers):
            # template= """You are provided with two responses for a given question below. First response is ground truth. Second response is generated by LLM.\
            #     Your task is to: 1. Evaluate the second response against first response and provide verdict as 'acceptable' or 'not acceptable'. 2. Provide a single line short explainantion.\
            #     Question: {question}\
            #     First response: {expected_answer} \
            #     Second response: {llm_answer}\
            #     Evaluation:"""
            template="""Evaluation Task: You are given two responses to a specific question. \
        The first response is the ground truth, considered as the accurate answer.\ 
        The second response has been generated by a language learning model (LLM).\ 
        Your task is to assess the quality of the LLM-generated response in comparison to the ground truth.\n

        Please use the following criteria for your evaluation:\n
        - Relevance: Does the LLM response directly address the question?\n
        - Accuracy Is the information provided in the LLM response correct?\n
        - Completeness: Does the LLM response cover the key points mentioned in the ground truth?\n\n

        Based on these criteria, assign a numerical score between 1 and 3, where:\n
        - 1 indicates a poor response (the LLM response is largely irrelevant, inaccurate, or incomplete),\n
        - 2 means an average response (the LLM response is somewhat relevant and accurate but may lack details or contain minor errors),\n
        - 3 signifies a good response (the LLM response is relevant, accurate, and complete).\n

        Question: {question}
        Ground Truth Response: {expected_answer}
        LLM-Generated Response: {llm_answer}
        Evaluation Score: """
            prompt = PromptTemplate(template=template, input_variables=["question", "expected_answer", "llm_answer"])
            llm_chain = LLMChain(prompt=prompt, llm=watsonx_llm)
            eval=llm_chain.predict(question=question, expected_answer=expected_answer, llm_answer=llm_answer )
            llm_evals.append(eval)
            await asyncio.sleep(1)
        return llm_evals
    # except ApiException as e:
    #     raise Exception(f"Watsonx AI API error: {e.message}")
    except Exception as e:
        raise Exception(f"Error generating answers: {e}")
    
    
