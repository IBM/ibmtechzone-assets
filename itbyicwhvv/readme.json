{"id":"itbyicwhvv","title":"Supervised Fine-tuning of IBM granite model using transformers","documentationURL":"https://github.ibm.com/Shilpa-Hegde2/Supervised_finetuning/blob/main/README.md","author":"Shilpa Hegde, Krish Hashia","categoryId":8,"description":"This asset focuses on the method of fine tuning the IBM Granite LLM to create a specialised LLM using transformer technique - LORA. In order ease the process of fine-tuning there is an autotune function which will determine the best parameters for the provided dataset without the user performing a trial and error in choosing the best parameters. This also shows how to evaluate the results generated before and after fine-tuning the model using the BERT embeddings-based similarity metrics where the results can easily be downloaded on the local machine. ","typeId":1,"type":"python","fileName":"","source":"git","assetURL":"https://github.ibm.com/Shilpa-Hegde2/Supervised_finetuning/blob/4d7e7d5f7f172a1baae9f380c6cd9e5d4f4e3c13/finetuning_ibm_granite7B.ipynb","userId":"7796dd6c-e3a6-4f46-a2bc-6292629199c0","preRequisites":{"fields":[],"requiredFields":[[]],"installDependencyCommands":["This notebook needs cuda support, this can also be run on T4 GPU in google colab notebook"]},"createdTimestamp":1724827779386,"updatedTimestamp":1724827779386}