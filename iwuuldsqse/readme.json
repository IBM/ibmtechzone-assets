{"id":"iwuuldsqse","title":"vLLM for serving LLMs on CPU","documentationURL":"https://github.ibm.com/Gautam-Chutani/LLM-Serving-Engines/tree/main/vLLM","author":"Gautam Chutani","categoryId":1,"description":"This script shows how to run multimodal models like LLaVA using vLLM on VM as standalone server with CPU-only machine.","typeId":1,"type":"python","fileName":"main.py","source":"git","assetURL":"https://github.ibm.com/Gautam-Chutani/LLM-Serving-Engines/tree/main/vLLM","userId":"3d9ff218-681e-4508-ac2e-9d2be7a8c35d","preRequisites":{"fields":[],"requiredFields":[[]],"installDependencyCommands":["echo \"Refer to GitHub Repository: https://github.ibm.com/Gautam-Chutani/LLM-Serving-Engines\""]},"createdTimestamp":1727016899363,"updatedTimestamp":1727016899363}