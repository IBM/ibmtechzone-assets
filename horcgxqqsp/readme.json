{"id":"horcgxqqsp","title":"MultiModal RAG with LLaVA VLM","documentationURL":"https://github.ibm.com/Gautam-Chutani/MultiModal-RAG","author":"Gautam Chutani, Shilpi Varshney","categoryId":1,"description":"This repository contains how to setup basic MultiModal RAG and other use-cases with LLaVa via vLLM on CPU and LLaVa-Next Vision Language Model available on watsonx.ai.\nIt also contains useful functions for image base64 format conversion.","typeId":1,"type":"python","fileName":"main.py","source":"git","assetURL":"https://github.ibm.com/Gautam-Chutani/MultiModal-RAG","userId":"3d9ff218-681e-4508-ac2e-9d2be7a8c35d","preRequisites":{"fields":[],"requiredFields":[[]],"installDependencyCommands":["echo \"Refer this GitHub Repository: https://github.ibm.com/Gautam-Chutani/MultiModal-RAG\""]},"createdTimestamp":1727126970282,"updatedTimestamp":1727127165323}