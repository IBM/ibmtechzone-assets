{"id":"gvujdkater","title":"Using \"Logprobs\" in LLM response ","author":"Gautam Chutani","categoryId":1,"description":"This code snippet demonstrates how we can make use of log probabilities (or logprobs parameter) in LLM API response for understanding the confidence level of generated text.\nIt includes code for both BAM and WatsonX.ai GA. \nAdditionally, one can also get the input and output token count using this code.","typeId":1,"type":"python","fileName":"gvujdkater.py","source":"local","assetURL":"","userId":"4038d64d-aa47-43c4-bad9-43c7cd3330d1","preRequisites":{"fields":[{"type":"text","label":"BAM_API_KEY"},{"type":"text","label":"WATSONX_API_KEY"},{"type":"text","label":"IBM_CLOUD_URL"},{"type":"text","label":"PROJECT_ID"}],"requiredFields":[["bam_api_key","watsonx_api_key","ibm_cloud_url","project_id"]],"installDependencyCommands":["pip install numpy","pip install ibm-generative-ai","pip install ibm-watson-machine-learning"]},"createdTimestamp":1716623575006,"updatedTimestamp":1716623575006}