import re
import pandas as pd
from bert_score import BERTScorer
from rouge import Rouge
scorer = BERTScorer(lang="en", rescale_with_baseline=True)

"""
This script demonstrates how to evaluate the performance of two models, LLAMA-70B and Mistral, by computing the average Bert and ROUGE scores for their generated answers compared to reference answers. 
It defines two functions: calculate_rouge_scores to compute ROUGE-L scores for model answers,and calculate_bert_scores for computing the bert scores and compare the scores of both models and select the model with the best performance. 
Example usage with dummy answers is provided, demonstrating how to use the functions to determine the best model.
"""

def clean_answer(text):
    return re.sub(r'||Answer:| | |\n|\\', '', text)

def create_eval_data(result_list, df):
    """
    This function returns a dataframe with index, question, answer, groundtruth and cleans the answers
    Args:
    - result_list : These are the answers generated from the LLM
    - df :You have golden question dataframe containing index, question, and groundtruth column
    """
    eval_data = pd.DataFrame()
    eval_data["index"] = range(1, len(df) + 1)
    eval_data["QUESTION"] = df["QUESTION"]
    eval_data["answer"] = result_list
    eval_data["ground_truth"] = df["GOLDEN ANSWER"]
    
    # Clean the answers
    eval_data["answer"] = eval_data["answer"].apply(clean_answer)
    eval_data["ground_truth"] = eval_data["ground_truth"].apply(clean_answer)
    
    return eval_data


def calculate_bert_score(row):
    """
    Calculate Bert Score for a given row containing 'answer' and 'ground_truth' column.
    """
    llama = row['answer']
    golden_answer = row['ground_truth']
    P, R, F1 = scorer.score([llama], [golden_answer], verbose=False)
    return P.item(), R.item(), F1.item()
   
def calculate_rouge_l_score(row):
    """
    Calculate ROUGE-L score for a given row containing 'answer' and 'ground_truth' columns.

    Args:
    - row: pandas Series representing a row of the DataFrame.

    Returns:
    - Returns the ROUGE-L score representing the similarity between the hypothesis and reference.
    """

    # Extract 'answer' and 'ground_truth' from the row
    hypothesis = row['answer']
    reference = row['ground_truth']

    # Initialize Rouge
    rouge = Rouge()

    # Calculate ROUGE scores
    scores = rouge.get_scores(hypothesis, reference, avg=True)
    
    # Extract ROUGE-L score
    rouge_p = scores['rouge-l']['p']
    rouge_r = scores['rouge-l']['r']
    rouge_f1 = scores['rouge-l']['f']
    
    return rouge_p,rouge_r,rouge_f1

def get_answers_from_llama():
    """
    Replace these with the answers generated by LLAMA model
    """
    result_list_llama = [
    "The fluffy marshmallow danced with the singing toaster.",
    "Purple monkeys played chess on the moon with rubber chickens.",
    "Sparkling diamonds rained from the sky while the clouds giggled."
]
    return result_list_llama

def get_answers_from_mistral():
    """
    Replace these with the answers generated by LLAMA model
    """
    result_list_mistral=[
    "The fluffy marshmallow danced with the singing toaster.",
    "Purple monkeys played chess on the moon with rubber chickens.",
    "Sparkling diamonds rained from the sky while the clouds giggled."
]
    return result_list_mistral



def get_average_scores(eval_data):
    """
    Calculate the average score
    """
    eval_data["Total_Score"] = eval_data["BERT_Precision"] + eval_data["BERT_Recall"] + eval_data["BERT_F1"] + eval_data["Rouge_Precision"] + eval_data["Rouge_Recall"] + eval_data["Rouge_F1"]
    avg_total_score = eval_data["Total_Score"].mean()
    print("Average combined score:", avg_total_score)
    return avg_total_score

if __name__ == "__main__":

    golden_data = pd.read_csv("Golden_questions.csv",index=False)
    result_list_llama =   get_answers_from_llama()
    result_list_mistral = get_answers_from_mistral()
    eval_data_llama = create_eval_data(result_list_llama, golden_data)
    eval_data_mistral = create_eval_data(result_list_mistral, golden_data)
    
    eval_data_llama[["BERT_Precision","BERT_Recall","BERT_F1"]] = eval_data_llama.apply(calculate_bert_score, axis=1, result_type='expand')
    eval_data_llama[["Rouge_Precision","Rouge_Recall","Rouge_F1"]] = eval_data_mistral.apply(calculate_rouge_l_score, axis=1, result_type='expand')
    eval_data_mistral[["BERT_Precision","BERT_Recall","BERT_F1"]] = eval_data_mistral.apply(calculate_bert_score, axis=1, result_type='expand')
    eval_data_mistral[["Rouge_Precision","Rouge_Recall","Rouge_F1"]] = eval_data_mistral.apply(calculate_rouge_l_score, axis=1, result_type='expand')

    avg_score_llama = get_average_scores(eval_data_llama)
    avg_score_mistral = get_average_scores(eval_data_mistral)

    if avg_score_llama > avg_score_mistral:
        print("The model with higher average score is: llama")
    elif avg_score_mistral > avg_score_llama:
        print("The model with higher average score is: mistral")
    else:
        print("Both models have the same average score.")





